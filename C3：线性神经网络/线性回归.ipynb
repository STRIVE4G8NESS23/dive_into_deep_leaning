{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNymng4/p+S+hiZfs7iH+2f"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["pip install d2l==0.17.6"],"metadata":{"id":"nOWt6PxaFrPi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pip uninstall matplotlib\n"],"metadata":{"id":"mjivRIBGI5l8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pip install --upgrade pip"],"metadata":{"id":"zREhKD6iJBQr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","pip install matplotlib"],"metadata":{"id":"YNw6J4d7JKgD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","2.4.1 导数绘图"],"metadata":{"id":"AYyesbsDJfpv"}},{"cell_type":"code","source":["%matplotlib inline\n","import numpy as np\n","from matplotlib_inline import backend_inline\n","from d2l import torch as d2l\n","\n","def f(x):\n","  return 3*x**2-4*x\n","\n","def numerical_lim(f,x,h):\n","  return (f(x+h)-f(x))/h\n","\n","h=0.1\n","for i in range(5):\n","  print(f'h={h:.5f}, numerical limit={numerical_lim(f,1,h):.5f}')\n","  h*=0.1\n","\n","def use_svg_display():\n","  backend_inline.set_matplotlib_formats('svg')\n","\n","def set_figsize(figsize=(3.5, 2.5)):\n","  use_svg_display()\n","  d2l.plt.rcParams['figure.figsize'] = figsize\n","\n","def set_axes(axes, xlabel, ylabel, xlim, ylim, xscale, yscale, legend):\n","  axes.set_xlabel(xlabel)\n","  axes.set_ylabel(ylabel)\n","  axes.set_xlim(xlim)\n","  axes.set_ylim(ylim)\n","  axes.set_xscale(xscale)\n","  axes.set_yscale(yscale)\n","  if legend:\n","    axes.legend(legend)\n","  axes.grid()\n","\n","def plot(X, Y=None, xlabel=None, ylabel=None, legend=None, xlim=None, ylim=None, xscale='linear', yscale='linear', fmts=('-', 'm--', 'g-.', 'r:'), figsize=(3.5, 2.5), axes=None):\n","  if legend is None:\n","    legend=[]\n","\n","  set_figsize(figsize)\n","  axes = axes if axes else d2l.plt.gca()\n","\n","  def has_one_axis(X):\n","    return (hasattr(X, \"ndim\") and X.ndim == 1 or isinstance(X, list) and not hasattr(X[0], \"__len__\"))\n","\n","  if has_one_axis(X):\n","    X=[X]\n","  if Y is None:\n","    X, Y = [[]] * len(X), X\n","  elif has_one_axis(Y):\n","    Y = [Y]\n","  if len(X) != len(Y):\n","    X = X*len(Y)\n","  axes.cla()\n","  for x, y, fmt in zip(X, Y, fmts):\n","    if len(x):\n","      axes.plot(x, y, fmt)\n","    else:\n","      axes.plot(y, fmt)\n","  set_axes(axes, xlabel, ylabel, xlim, ylim, xscale, yscale, legend)\n","\n","x = np.arange(0, 3, 0.1)\n","plot(x, [f(x), 2*x-3], 'x', 'f(x)', legend=['f(x)', 'Tangent line (x=1)'])\n"],"metadata":{"id":"BbSt-bjF_Lxu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["2.6.1 概率"],"metadata":{"id":"F084dqPbJrtR"}},{"cell_type":"code","source":["%matplotlib inline\n","import torch\n","from torch.distributions import multinomial\n","from d2l import torch as d2l\n","\n","fair_probs = torch.ones([6])/6\n","# multinomial.Multinomial(1, fair_probs).sample()\n","# multinomial.Multinomial(10, fair_probs).sample()\n","\n","# count = multinomial.Multinomial(1000, fair_probs).sample()\n","# count/1000\n","\n","counts = multinomial.Multinomial(10, fair_probs).sample((500,))\n","cum_counts = counts.cumsum(dim=0)\n","estimates = cum_counts/cum_counts.sum(dim=1, keepdims=True)\n","\n","d2l.set_figsize((6,4.5))\n","for i in range(6):\n","  d2l.plt.plot(estimates[:,i].numpy(), label=(\"P(die=\" + str(i+1) + \")\"))\n","d2l.plt.axhline(y=0.167, color='black', linestyle='dashed')\n","d2l.plt.gca().set_xlabel('Groups of experiments')\n","d2l.plt.gca().set_ylabel('Estimated probability')\n","d2l.plt.legend()\n"],"metadata":{"id":"PRuwNPfrIfEI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["解析解："],"metadata":{"id":"qfAbtwQFFg7H"}},{"cell_type":"code","source":["import numpy as np\n","import matplotlib.pyplot as plt\n","# 100行1列的x1\n","X = 2 * np.random.rand(100, 1)\n","#这个公式就等于： 真实的y=预测的y+误差；预测的y=x0*a+b*x，因为x0恒为1，所以就是a+b*x\n","y = 4 + 3 * X + np.random.randn(100, 1)\n","# 整合x0和x1 x0是100行1\n","X_b = np.c_[np.ones((100, 1)), X]\n","\n","#用解析解公式求theta\n","# linalg是线形和代数这两个英文词组合起来的，inv是在求逆，dot是点乘，T是求转置\n","theta_best = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y)\n","\n","print(theta_best)\n","\n","# 但是在机器学习中数据为王的思想，数据特别多，不适合上面那种方法，太慢了，\n","# 预测\n","X_new = np.array([[0], [2]])# 2行一列的数组x1，值为0，2\n","X_b_new = np.c_[(np.ones((2, 1))), X_new] #x0，x1整合\n","print(X_b_new)\n","# 根据上面求得的模型theta预测未来\n","y_predict = X_b_new.dot(theta_best)\n","#将0，和2分别带入预测的y=4+3*x，看得到的值是否和y_predict差不多，肯定会有误差的，接近就行\n","print(y_predict)\n","\n","plt.plot(X_new, y_predict, 'r-')\n","plt.plot(X, y, 'b.')\n","plt.axis([0, 2, 0, 15])\n","plt.show()"],"metadata":{"id":"mmEX-lFSFTvq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["3.1.2 向量化加速"],"metadata":{"id":"UvQDYBPzN0Zy"}},{"cell_type":"code","source":["%matplotlib inline\n","import math\n","import time\n","import numpy as np\n","import torch\n","from d2l import torch as d2l\n","\n","n = 10000\n","a = torch.ones(n)\n","b = torch.ones(n)\n","\n","# 定义一个计时器\n","class Timer:\n","  def __init__(self):\n","    self.times = []\n","    self.start()\n","    pass\n","\n","  def start(self):\n","    #启动计时器\n","    self.tik = time.time()\n","\n","  def stop(self):\n","    #停止计时器并将时间记录在列表中\n","    self.times.append(time.time() - self.tik)\n","    return self.times[-1]\n","\n","  def avg(self):\n","    #返回平均时间\n","    return sum(self.times)/len(self.times)\n","\n","  def sum(self):\n","    #返回时间总和\n","    return sum(self.times)\n","\n","  def cumsum(self):\n","    #返回累计时间\n","    return np.array(self.times).cumsum().tolist()\n","\n","c = torch.zeros(n)\n","timer = Timer()\n","for i in range(n):\n","  c[i] = a[i] + b[i]\n","f'{timer.stop():.5f} sec'\n","\n","timer.start()\n","d = a + b\n","f'{timer.stop():.5f} sec'"],"metadata":{"id":"TuSObas1N5gm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["3.2 线性回归"],"metadata":{"id":"Bfa-NqGujMZK"}},{"cell_type":"code","source":["from numpy.linalg import LinAlgError\n","%matplotlib inline\n","import torch\n","import random\n","from d2l import torch as d2l\n","\n","#生成数据集\n","def synthetic_data(w, b, num_examples):\n","  '''生成y=Xw+b+噪声'''\n","  X = torch.normal(mean=0, std=1, size=(num_examples, len(w)))\n","  y = torch.matmul(X, w) + b\n","  y += torch.normal(mean=0, std=0.01, size=y.shape)\n","  return X, y.reshape(-1,1)\n","\n","true_w = torch.tensor([2, -3.4])\n","true_b = 4.2\n","features, labels = synthetic_data(true_w, true_b, 1000)\n","\n","print('features:', features[0], '\\nlables:', labels[0])\n","\n","d2l.set_figsize()\n","d2l.plt.scatter(features[:, 1].detach().numpy(), labels.detach().numpy(), 1);\n","\n","#读取数据集\n","def data_iter(batch_size, features, labels):\n","  num_examples = len(features)\n","  indices = list(range(num_examples))\n","  # 这些样本是随机读取的，没有特定的顺序\n","  random.shuffle(indices) # 随机打乱\n","  for i in range(0, num_examples, batch_size):\n","    batch_indices = torch.tensor(indices[i:min(i+batch_size, num_examples)])\n","    yield features[batch_indices], labels[batch_indices] # 生成器，在for循环中迭代\n","\n","batch_size = 10\n","for X, y in data_iter(batch_size, features, labels):\n","  print(X, '\\n', y)\n","  break\n","\n","# 初始化模型参数\n","w = torch.normal(mean=0, std=0.01, size=(2,1), requires_grad=True)\n","b = torch.zeros(1, requires_grad=True)\n","\n","#定义模型\n","def linreg(X, w, b):\n","  '''线性回归模型'''\n","  return torch.matmul(X, w) + b\n","\n","# 定义损失函数\n","def squared_loss(y_hat, y):\n","  return (y_hat-y.reshape(y_hat.shape)) ** 2 / 2\n","\n","# 定义优化方法\n","def sgd(params, lr, batch_size):\n","  '''小批量随机梯度下降'''\n","  with torch.no_grad():\n","    for param in params:\n","      param -= lr * param.grad/batch_size\n","      param.grad.zero_()\n","\n","# 训练\n","lr = 0.03 # 学习率\n","num_epochs = 3 # 轮数\n","net = linreg\n","loss = squared_loss\n","\n","for epoch in range(num_epochs):\n","  for X, y in data_iter(batch_size, features, labels):\n","    l = loss(net(X, w, b), y) # X和y的小批量损失\n","    l.sum().backward()\n","    sgd([w,b], lr, batch_size) # 使用参数的梯度更新参数\n","  with torch.no_grad():\n","    train_l = loss(net(features, w, b), labels)\n","    print(f'epoch{epoch+1}, loss {float(train_l.mean()):f}')\n","\n","print(f'w的估计误差：{true_w - w.reshape(true_w.shape)}')\n","print(f'b的估计误差：{true_b - b}')"],"metadata":{"id":"BJ5ROip0jPq9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["3.3 线性回归简洁实现"],"metadata":{"id":"dTtG--gm0U6G"}},{"cell_type":"code","source":["import numpy as np\n","import torch\n","from torch.utils import data\n","from torch import nn\n","from d2l import torch as d2l\n","\n","# 生成数据集\n","true_w = torch.tensor([2, -3.4])\n","true_b = 4.2\n","features, labels = d2l.synthetic_data(true_w, true_b, 1000)\n","\n","# 读取数据集\n","def load_array(data_arrays, batch_size, is_train=True):\n","  '''构造一个Pytorch迭代器'''\n","  dataset = data.TensorDataset(*data_arrays)\n","  return data.DataLoader(dataset, batch_size, shuffle=is_train)\n","\n","batch_size = 10\n","data_iter = load_array((features, labels), batch_size)\n","next(iter(data_iter))\n","\n","# 定义模型\n","net = nn.Sequential(nn.Linear(2,1)) # 2表示输入特征形状，1表示输出特征形状\n","\n","# 初始化模型参数\n","net[0].weight.data.normal_(mean=0,std=0.01)\n","net[0].bias.data.fill_(0)\n","\n","# 定义损失函数\n","loss = nn.MSELoss() # 平方L2范数\n","\n","# 定义优化方法\n","trainer = torch.optim.SGD(net.parameters(), lr=0.03)\n","\n","# 训练\n","num_epochs = 3\n","for epoch in range(num_epochs):\n","  for X, y in data_iter:\n","    l = loss(net(X), y)\n","    trainer.zero_grad()\n","    l.backward()\n","    trainer.step()\n","  l = loss(net(features), labels)\n","  print(f'epoch {epoch+1}, loss {l:f}')\n","\n","w = net[0].weight.data\n","print('w的估计误差：', true_w - w.reshape(true_w.shape))\n","b = net[0].bias.data\n","print('b的估计误差：', true_b - b)"],"metadata":{"id":"z7584ip-0ast"},"execution_count":null,"outputs":[]}]}